In bytecode, each instruction is only a byte long
Source Code --(compiler)--> Bytecode --(VM)--> Machine code
In bytecode format, each instruction has one-byte opcode (ADD, SUB, DIV, PUSH etc)

Stages of interpretation for clox
Scanner --(Token)-> Compiler --(Chunk)--> VM
Source Code --(Scanner)--> Tokens --(Compiler)--> Chunks --(VM)--> OK!! 

In clox, we donot have the luxury to scan the whole code eagerly since we donot have access to a dynamic list in C (Very memory wasteful), so since our grammar needs only one or max 2 tokens at a time, we can wait until a token is needed before we scan it.

In C printf("%.<n>s"); where n = 1,2,3 ... is the maximum of characters to show. if n = *, then we need to pass the number as parameter e.g: printf("%.*s", 3);
 
In clox, token store their begining index of their lexeme and then the length of the lexeme as it is. We donot cast the lexeme to a particular type until we reach compilation. This is to avoid memory overuse

Hashmaps are overkill in C to look for keyword identifiers, so we will use a Regular Expression. We will built and Deterministic Finit Automaton (DFA) that recognizes keywords of lox

Notice, we could just build a large DFA that recognizes and spits out tokens from lox using a tool called `Lex` instead of a scanner.

Compiler = Parser + Code Generation

Precedence is very important in parsing expressions remember BODMAS. When you finish parsing an operator, the ideal thing is to continue parsing only expressions which are greater than it.

The compiler always advances past ecah instruction before executing it

When we add and Operation Opcode in (chunk.h), we add the when to call it in the compiler.c's rules array, then we modify the corresponding infix or prefix function, then we 

Functions, Strings and classe types are stored in the heap and not the stack because they have a variable size 

============================HASH TABLES=====================================
- Our hashmap or hashtable or dictionnary or ... is simply an array of buckets(empty cells).
- For every value, we produce a constant hash integer, which can be used as an index of the bucket to find the value. Usually, we take `hash mod(Number of buckets)` and so have risk of collisions since we use modulus. 
- Each array cell can be seen as a bucket
- The load factor is the value of the ratio `Number of entries`: `Number of buckets`
- Collision resolving techniques include 
1) Separate Chaining, in which the bucket points to a linked list containing in an unordered manner all values which collided. So index 3 points to a list containing all value whose hash was three and we need to parcour the list to get the value, somehow
2) Open Addressing or Closed Hashing, Here if a collisions is found, we find the next empty bucket in our array and fill it and if we reach the end of the array, we coil back at to the first element and continue our search and fail when we reach our starting point. This method is CPUcache friendly. The process of parcouring to find an empty cell is called Linear probing. But how do we get back the value ?????

 A tombstone entry is one which is added to a bucket after a deletion is performed on the bucket. This is to delete without destroying probing

 String interning is an interesting concept in that, we make similar strings point to the same memory location and thus making comparison trivial. We use tables for that.

Notice that statments do not produce any value and so they leave the stack unchanged. While, expressions combines values to produce a single result and thus changes the stack. 
